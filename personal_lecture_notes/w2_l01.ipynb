{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vecka 2\n",
    "\n",
    "sammanfatta linjär regression(allmän linjär regression)\n",
    "\n",
    "gradient descent, maximum likelihood\n",
    "\n",
    "reguljärisering\n",
    "\n",
    "korsvalidering\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# allmän linjär regression\n",
    "\n",
    "här ger vi upp statistiken pga kolinjäritet, därför behövs regulärisering\n",
    "\n",
    "vi söker linjära approximationer givet ett stickprov(träningsdata)\n",
    "\n",
    "$Y = \\beta_{0}+\\beta_{1}X_{1}+\\epsilon$\n",
    "\n",
    "$\\beta_{0}\\beta_{1}$ skall bestämmas\n",
    "\n",
    "$min C(B)=\\sum\\limits_{i=1}^{n} (y_{i}-\\beta_{0}-\\beta_{1}X_{1}) = \\sum\\limits_{i=1}^{n}(y_{i}-\\hat{y})$\n",
    "\n",
    "optimering : $\\frac{1}{n}\\sum\\limits_{i=1}^{n}(y_{i}-\\hat{y}$\n",
    "\n",
    "hur funkar det?\n",
    "\n",
    "derivata $\\implies$ sätt till 0 $\\implies$ analys\n",
    "\n",
    "$\\epsilon^2$ står kvar i högerledet, dvs vi optimerar mot låg varians\n",
    "\n",
    "det innebär vi hittar ett väntevärde för Y givet X är linjärt beroende till X: $E[Y|X]=XB$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "när vi går upp i en till dimension så har vi istället ett plan för en linje, nu fördelar punkterna kring planet istället.\n",
    "\n",
    "lutningen på ett plan är inte längre en skalär, utan nu kallas det för en gradient $(\\frac{\\partial}{\\partial x_{1}},\\frac{\\partial}{\\partial x_{2}})= \\Delta f(x_{1},x_{2})$\n",
    "\n",
    "gradienten pekar alltid i den riktning där förändringshastigheten är störst\n",
    "\n",
    "$Y = XB + E$  \n",
    "\n",
    "feature-expansion för att lämna strikt linjära förhållanden $Y=\\beta_0+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\beta_{3}X_{1}X_{2}$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y=\\beta_{1}X_{1}X_{2}$ <- blir en s.k saddle point yta, vi förväntar inte ett linjärt förhållande\n",
    "\n",
    "$E[Y|X] = f(X)$\n",
    "\n",
    "polynom-expansion:\n",
    "$Y=\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+\\beta_{3}y^{2}\\beta_{4}{x_{2}}^2+\\beta_{5}x_{1}x_{2}$\n",
    "\n",
    "\n",
    "länkfunktionen är polynomet\n",
    "\n",
    "sökord\n",
    "\n",
    "kapitel 2 & 3 i boken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Accuracy & precision\n",
    "\n",
    "bias är inversen av accuracy - låg: långt ifrån medlet\n",
    "\n",
    "varians är inversen till precision - hög: spridning högre spridning\n",
    "\n",
    "man vill optimera mot låg varians, alltså hög precision\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generell(generativ) additiv modell | kapitel 7 i boken\n",
    "\n",
    "$Y=f(x_{0})+f(x_{1})+...+f_d(X_{d})(+\\epsilon)$\n",
    "\n",
    "$\\implies$ om det är så att funktionerna har formen $\\sum\\limits_{i=1}^{n}x_{0}f_{0}(x_{0})\\implies \\sum\\limits_{i=1}^{n}x_{0}f(x)$\n",
    "då är detta en distribution, då bestämmer funktionen f(x)(i sista ekvationen) vilken distribution vi har\n",
    "\n",
    "nu är det inte längre närmrevärdet vi är intresserade av, och istället sannolikheten att Y givet X ($\\approx Pr(Y|X)$)\n",
    "\n",
    "vi ska ha en kostnadsfunktion och en s.k länkfunktion eftersom $E[X]=\\mu$\n",
    "\n",
    "länkfunktionen för en linjär regression är medlet $\\mu$\n",
    "\n",
    "\n",
    "när det gäller ett klassifieringsproblem är ser vi det som en serie oberoende försök (bernoulli försök) med viss sannolikhet att tillhöra klassen eller inte\n",
    "\n",
    "OLS är ett specialfall av maximum likelihood(linear discriminant finns om i boken ksk kap 4)\n",
    "\n",
    "vi nöjer oss dock med logistisk regression\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ortogonalisering är det vi gör när vi har kolinjäritet\n",
    "\n",
    "det betyder att vi tar våra variabler och hittar en ny bas för dem\n",
    "\n",
    "det vi försöker göra är att hitta en riktig bas för datan så att dem är oberoende från varandra\n",
    "\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "varians och bias\n",
    "\n",
    "$MSE = Bias^2+varians+brus(irreducibel del)$\n",
    "\n",
    "vi försöker minska variansen, vi kan oftast inte göra något med bias för att komma åt grundsanningen, det är nån typ av konstant, minskar vi bias ökar varians o vice versa.\n",
    "\n",
    "regulärisering minskar varians men tenderar att öka bias\n",
    "\n",
    "vi vill undvika 2 situationer:\n",
    "\n",
    "underfit: för enkel modell, låg varians, högt bias(hög precision men låg nogrannhet) litet konfidensintervall\n",
    "\n",
    "exempel $9.3127891113\\pm8.2$\n",
    "\n",
    "overfit: för komplicerad modell, hög varians, lågt bias, (låg precision men hög noggranhet)stort konfidensintervall\n",
    "\n",
    "exempel $9.1 \\pm 0.00134$\n",
    "\n",
    "\n",
    "varians är den som i de flesta fall skall minskas\n",
    "\n",
    "regulärisering $\\implies$ ändra kostnadsfunktionen, så att vi straffar lösningen som vi inte vill ha\n",
    "\n",
    "dimensionsreducering $->$ feature-mängden\n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# regulärisering\n",
    "\n",
    "vi behöver ta hänsyn till varians i datan, vi behöver göra det mer sannolikt eftersom vi inte kan göra forward och backward selection\n",
    "\n",
    "detta har o göra med topologi, ridge regression/l2 norm, euclidiskt avstånd och kvadrater osv.\n",
    "\n",
    "\n",
    "#### vi har en kostnadsfunktion:\n",
    " ridge regression\n",
    "\n",
    "$min C(\\beta)= RSS + \\lambda \\sum\\limits_{i=1}^{d}{\\beta_{d}}^2$ : l2-norm vi straffar stora värden på koeffs\n",
    "\n",
    "vi sprider ut värdena, minskar variansen genom att tvinga beta värdena att försöker göra dem så små som möjligt,\n",
    "\n",
    "vi behöver utvärdera med MSE\n",
    "\n",
    "#### ett ennat sätt att göra det på: \n",
    "lasso regression\n",
    "\n",
    "$min C(\\beta)= RSS + \\lambda \\sum\\limits_{i=1}^{d}|\\beta_d|$ : l1-norm\n",
    "\n",
    "$\\lambda$ = hur stark effekten är, bästa $\\lambda$ går att hitta med hyperparameter-optimisering\n",
    "\n",
    "\n",
    "denna tenderar att sätta beta parametrar till 0 om dem inte är signifikanta\n",
    "\n",
    "helst vill vi göra båda dessa\n",
    "\n",
    "-\n",
    "\n",
    "#### elasticnet regression\n",
    "\n",
    "$min C(\\beta)= RSS + \\lambda (\\frac{1-\\alpha}{2}\\sum\\limits_{i=1}^{d}{\\beta_{i}}^2\\pm \\alpha \\sum\\limits_{i=1}^{d}|\\beta_{i}|)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$min C(\\beta)= RSS + \\lambda (\\frac{1-\\alpha}{2}\\sum\\limits_{i=1}^{d}{\\beta_{i}}^2\\pm \\alpha \\sum\\limits_{i=1}^{d}|\\beta_{i}|)$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
