{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient descent och klassifiering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skalering\n",
    "$Y=\\beta_{0}+\\beta_{1}X_{1}+B_{2}X_{2}$\n",
    "\n",
    "vad är en skala?\n",
    "min -------  max \n",
    "\n",
    "ett sätt: min-max skalering\n",
    "- känsligt för extremvärden / outliers\n",
    "\n",
    "det andra sättet: \n",
    "- standardisering: \n",
    " $\\frac{X-\\mu}{\\sigma}$\n",
    "- det kan ses som en normalisering, man centrerar alla värden runt nollan\n",
    "- varför vill man göra detta? om $\\beta_{1}$ har väldigt små värden och $\\beta_{2}$ har väldigt stora värden t.ex så dem får samma betydelse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regulärisering\n",
    "$C(B) = RSS + \\sum\\limits_{i=1}^{d}{\\beta_{i}}^2$ -  ridge regression, måste ha $\\beta_{i}$ på samma skala.\n",
    "\n",
    "- om 0 ska ha betydelsen \"ingenting\" så behövs också skalering.\n",
    "\n",
    "Y skaleras oftast inte!\n",
    "- anledningen är att vi vill behålla storleken på responsen, t.ex om Y representerar en verklig summa som till exempel pengar så är det viktigt att enheten är enhetlig!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### minsta kvadratmetoden är ett specialfall av maximum likelihood\n",
    "\n",
    "$max C(B) = \\Pi P \\Pi 1-P$ (en klass)\n",
    "då fungerar inte längre formeln för våra $\\beta$ som vi hade för linjär regression\n",
    "\n",
    "vi måste hitta ett annat sätt att optimera över kostnadsfunktionen!\n",
    "\n",
    "olika sätt att göra det på:\n",
    "- Gradient descent! i allmänhet, man har nästan gett upp alla andra metoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient descent - iterativ metod\n",
    "\n",
    "hitta gradienterna iterativt, \n",
    "\n",
    "tar små steg och tittar på gradienterna tills vi hittara lägsta punkten\n",
    "\n",
    "- steglängd är viktigt\n",
    "- om ytan är \"knagglig\" så hoppar punkten omkring väldigt mycket och konvergerar inte.(instabil)\n",
    "--\n",
    "\n",
    "- en lösning på detta är SGD - stocastic gradient descent\n",
    "- vi väljer här EN slumpmässig punkt, nu konvergerar inte metoden, vi måste ha ett stoppvillkor.\n",
    "- den fastnar dock inte i lokala minimum\n",
    "\n",
    "  .\n",
    "\n",
    "mini batch gradient descent - iterativ metod\n",
    "- inte längre en analytisk metod - kostnadsfunktionen ser helt annorlunda ut, vi behöver ett stoppvillkor och varje iteration kallas för en epok, iterativa hamnar lätt i kaos\n",
    "- här väljer vi en slumpmässig delmängd räknar gradienten på\n",
    "\n",
    "$X_{n+1} = 3.58(1-X_n)$: logistic map - kaotisk fraktal,\n",
    "så när man kör stocastic gradient descent så kan det flippa ur, man försöker arrangera ett kaos med andra ord.\n",
    "\n",
    "man behöver justera några parametrar för att SGD skall bli bra - steglängden behöver förändras under körningen, i början behöver vi stora steg och i slutet små steg!  - Adaptive gradient descent.\n",
    "\n",
    "då får vi något som kallas ADAM: Adaptive moment estimation,\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
