{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"https://colab.research.google.com/github/kokchun/Machine-learning-AI22/blob/main/Exercises/E01_gradient_descent.ipynb\" target=\"_parent\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; to see hints and answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Gradient descent exercises\n",
    "\n",
    "---\n",
    "These are introductory exercises in Machine learning with focus in **gradient descent** .\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Note</b> all datasets used in this exercise can be found under Data folder of the course Github repo</p>\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Note</b> that in cases when you start to repeat code, try not to. Create functions to reuse code instead. </p>\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Remember</b> to use <b>descriptive variable, function, index </b> and <b> column names</b> in order to get readable code </p>\n",
    "\n",
    "The number of stars (\\*), (\\*\\*), (\\*\\*\\*) denotes the difficulty level of the task\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Simulate dataset (*)\n",
    "\n",
    "Simulate datasets according to these rules:\n",
    "\n",
    "- set random seed to 42\n",
    "- (1000,2) samples from $X \\sim \\mathcal{U}(0,1)$ , i.e. 1000 rows, 2 columns. \n",
    "- 1000 samples from $\\epsilon \\sim \\mathcal{N}(0,1)$\n",
    "- $y = 3x_1 + 5x_2 + 3 + \\epsilon$ , where $x_i$ is column $i$ of $X$\n",
    "\n",
    "Finally add a column of ones for the intercept to $X$.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Use for simulating X\n",
    "\n",
    "´´´\n",
    "np.random.rand(samples, 2)\n",
    "´´´\n",
    "\n",
    "to concatenate with ones, use ```np.c_[..., ...]```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Answer</summary>\n",
    "\n",
    "```\n",
    "array([[1.        , 0.37454012, 0.95071431],\n",
    "       [1.        , 0.73199394, 0.59865848],\n",
    "       [1.        , 0.15601864, 0.15599452],\n",
    "       [1.        , 0.05808361, 0.86617615],\n",
    "       [1.        , 0.60111501, 0.70807258]])\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 3), (1000, 1))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.uniform(0,1,(1000,2))\n",
    "eps = np.random.normal(0,1,1000)\n",
    "\n",
    "\n",
    "y = 3*X[:,0] + 5*X[:,1] + 3 + eps\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "X = np.column_stack([np.ones(X.shape[0]),X])\n",
    "\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient descent - learning rate (*)\n",
    "\n",
    "Use gradient descent to calculate $\\vec{\\theta} = (\\theta_0, \\theta_1, \\theta_2)^T$ \n",
    "\n",
    "&nbsp; a) Use $\\eta = 0.1$ and calculate $\\vec{\\theta}$ for each fifth epoch from 1 to 500. So the procedure is as follows:\n",
    "- calculate $\\vec{\\theta}$ for epochs = 1\n",
    "- calculate $\\vec{\\theta}$ for epochs = 6\n",
    "- ...\n",
    "- calculate $\\vec{\\theta}$ for epochs = 496\n",
    "\n",
    "Plot these $\\vec{\\theta}$ values against epochs. (*)\n",
    "\n",
    "&nbsp; b) Do the same as for a) but with learning rate $\\eta = 0.01$, 5000 epochs and for each 20th epoch. What do you notice when changing the learning rate? (*)\n",
    "\n",
    "&nbsp; c) Experiment with larger and smaller $\\eta$ and see what happens.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Answer</summary>\n",
    "\n",
    "a) \n",
    "\n",
    "<img src=\"../assets/grad_desc_converg.png\" height=\"200\"/>\n",
    "\n",
    "b) \n",
    "\n",
    "<img src=\"../assets/grad_desc_converg_001.png\" height=\"200\"/>\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m steps2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m50001\u001b[39m,\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m     16\u001b[0m thetas1 \u001b[38;5;241m=\u001b[39m [gradient_descent(X,y,iterations\u001b[38;5;241m=\u001b[39mi)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m steps1]\n\u001b[0;32m---> 17\u001b[0m thetas2 \u001b[38;5;241m=\u001b[39m [\u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m steps2]\n\u001b[1;32m     19\u001b[0m thetas1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(thetas1)\n\u001b[1;32m     20\u001b[0m thetas2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(thetas2)\n",
      "Cell \u001b[0;32mIn[45], line 9\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(X, y, learning_rate, iterations)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[1;32m      8\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m/\u001b[39mm) \u001b[38;5;241m*\u001b[39mX\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m@\u001b[39m(X\u001b[38;5;129m@theta\u001b[39m\u001b[38;5;241m-\u001b[39my)\n\u001b[0;32m----> 9\u001b[0m     theta \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate\u001b[38;5;241m*\u001b[39mgradient\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m theta\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def gradient_descent(X, y, learning_rate = .1, iterations = 500):\n",
    "    m = len(X)\n",
    "\n",
    "    theta = np.random.randn(X.shape[1],1)\n",
    "    #print(f\"Initialize theta with randomized\\n {theta}\")\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        gradient = (2/m) *X.T@(X@theta-y)\n",
    "        theta -= learning_rate*gradient\n",
    "\n",
    "    return theta\n",
    "\n",
    "\n",
    "steps1 = range(1,501,5)\n",
    "steps2 = range(1,50001,200)\n",
    "thetas1 = [gradient_descent(X,y,iterations=i).reshape(-1) for i in steps1]\n",
    "thetas2 = [gradient_descent(X,y,learning_rate=0.01,iterations=i).reshape(-1) for i in steps2]\n",
    "\n",
    "thetas1 = np.array(thetas1)\n",
    "thetas2 = np.array(thetas2)\n",
    "\n",
    "thetas1\n",
    "thetas2\n",
    "\n",
    "\n",
    "def plot_gradient_descent(steps, thetas):\n",
    "    plt.figure(figsize=(10, 6)) # gpt plot lol\n",
    "    plt.plot(steps, thetas[:, 0], label='θ₀ (Intercept)')\n",
    "    plt.plot(steps, thetas[:, 1], label='θ₁')\n",
    "    plt.plot(steps, thetas[:, 2], label='θ₂')\n",
    "    plt.axhline(y=3, color='r', linestyle='--', label='True θ₀')\n",
    "    plt.axhline(y=3, color='g', linestyle='--', label='True θ₁')\n",
    "    plt.axhline(y=5, color='b', linestyle='--', label='True θ₂')\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.ylabel('Parameter values')\n",
    "    plt.title('Gradient Descent Convergence')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Let's also see the final values of our parameters\n",
    "    print(f\"Final theta values (after {steps[-1]} iterations):\")\n",
    "    print(f\"θ₀ = {thetas[-1, 0]:.4f}, True θ₀ = 3\")\n",
    "    print(f\"θ₁ = {thetas[-1, 1]:.4f}, True θ₁ = 3\")\n",
    "    print(f\"θ₂ = {thetas[-1, 2]:.4f}, True θ₂ = 5\")\n",
    "\n",
    "plot_gradient_descent(steps1, thetas1)\n",
    "plot_gradient_descent(steps2, thetas2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stochastic Gradient Descent - learning rate (**)\n",
    "\n",
    "Repeat task 1 but using stochastic gradient descent instead. Also adjust number of epochs to see if you can find convergence. What kind of conclusions can you draw from your experiments. (**)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,3) and (2,1) not aligned: 3 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m x_i \u001b[38;5;241m=\u001b[39m X[random_index:random_index\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# this gives an extra dimension on the matrix\u001b[39;00m\n\u001b[1;32m     16\u001b[0m y_i \u001b[38;5;241m=\u001b[39m y[random_index:random_index\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 17\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mx_i\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m error \u001b[38;5;241m=\u001b[39m prediction \u001b[38;5;241m-\u001b[39m y_i \n\u001b[1;32m     19\u001b[0m gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mx_i\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(error)\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,3) and (2,1) not aligned: 3 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "t0, t1 = 5, 50\n",
    "\n",
    "\n",
    "def learning_rate_schedule(t):\n",
    "    \"\"\"Decrease learning rate as training progresses\"\"\"\n",
    "    return t0/(t+t1)\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "m = len(X)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        x_i = X[random_index:random_index+1] # this gives an extra dimension on the matrix\n",
    "        y_i = y[random_index:random_index+1]\n",
    "        prediction = x_i.dot(theta)\n",
    "        error = prediction - y_i \n",
    "        gradients = 2*x_i.T.dot(error)\n",
    "        eta = learning_rate_schedule(epoch*m+i)\n",
    "        theta -= eta*gradients\n",
    "\n",
    "\n",
    "theta\n",
    "\n",
    "plot_gradient_descent(epochs,theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mini Batch Gradient Descent (**)\n",
    "\n",
    "Now try different sizes of mini-batches and make some exploratory plots to see convergence. Also you can make comparison to the other algorithms by using same $\\eta$ and same amount of epochs to see how they differ from each other in terms of convergence. (**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Kokchun Giang\n",
    "\n",
    "[LinkedIn][linkedIn_kokchun]\n",
    "\n",
    "[GitHub portfolio][github_portfolio]\n",
    "\n",
    "[linkedIn_kokchun]: https://www.linkedin.com/in/kokchungiang/\n",
    "[github_portfolio]: https://github.com/kokchun/Portfolio-Kokchun-Giang\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
